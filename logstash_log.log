Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties
[2024-09-24T08:33:15,175][INFO ][logstash.runner          ] Log4j configuration path used is: /usr/share/logstash/config/log4j2.properties
[2024-09-24T08:33:15,206][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"8.12.2", "jruby.version"=>"jruby 9.4.5.0 (3.1.4) 2023-11-02 1abae2700f OpenJDK 64-Bit Server VM 17.0.10+7 on 17.0.10+7 +indy +jit [x86_64-linux]"}
[2024-09-24T08:33:15,210][INFO ][logstash.runner          ] JVM bootstrap flags: [-XX:+HeapDumpOnOutOfMemoryError, -Dlogstash.jackson.stream-read-constraints.max-number-length=10000, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, -Djruby.regexp.interruptible=true, --add-opens=java.base/java.security=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, -Dio.netty.allocator.maxOrder=11, -Dlog4j2.isThreadContextMapInheritable=true, -Xms1g, -Dlogstash.jackson.stream-read-constraints.max-string-length=200000000, -Djdk.io.File.enableADS=true, -Dfile.encoding=UTF-8, --add-opens=java.base/java.io=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, -Djruby.compile.invokedynamic=true, -Xmx1g, -Djava.security.egd=file:/dev/urandom, -Djava.awt.headless=true, -Dls.cgroup.cpuacct.path.override=/, -Dls.cgroup.cpu.path.override=/, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED]
[2024-09-24T08:33:15,234][INFO ][logstash.runner          ] Jackson default value override `logstash.jackson.stream-read-constraints.max-string-length` configured to `200000000`
[2024-09-24T08:33:15,235][INFO ][logstash.runner          ] Jackson default value override `logstash.jackson.stream-read-constraints.max-number-length` configured to `10000`
[2024-09-24T08:33:17,264][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600, :ssl_enabled=>false}
[2024-09-24T08:33:17,975][INFO ][org.reflections.Reflections] Reflections took 212 ms to scan 1 urls, producing 132 keys and 468 values
[2024-09-24T08:33:18,598][INFO ][logstash.javapipeline    ] Pipeline `service_stamped_json_logs` is configured with `pipeline.ecs_compatibility: v8` setting. All plugins in this pipeline will default to `ecs_compatibility => v8` unless explicitly configured otherwise.
[2024-09-24T08:33:18,614][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//elasticsearch:9200"]}
[2024-09-24T08:33:19,152][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elasticsearch:9200/]}}
[2024-09-24T08:33:19,526][WARN ][logstash.outputs.elasticsearch][service_stamped_json_logs] Restored connection to ES instance {:url=>"http://elasticsearch:9200/"}
[2024-09-24T08:33:19,526][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Elasticsearch version determined (8.12.2) {:es_version=>8}
[2024-09-24T08:33:19,527][WARN ][logstash.outputs.elasticsearch][service_stamped_json_logs] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>8}
[2024-09-24T08:33:19,547][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Data streams auto configuration (`data_stream => auto` or unset) resolved to `true`
[2024-09-24T08:33:19,577][WARN ][logstash.filters.grok    ][service_stamped_json_logs] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-09-24T08:33:19,886][WARN ][logstash.javapipeline    ][service_stamped_json_logs] 'pipeline.ordered' is enabled and is likely less efficient, consider disabling if preserving event order is not necessary
[2024-09-24T08:33:19,896][INFO ][logstash.javapipeline    ][service_stamped_json_logs] Starting pipeline {:pipeline_id=>"service_stamped_json_logs", "pipeline.workers"=>1, "pipeline.batch.size"=>1, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1, "pipeline.sources"=>["/usr/share/logstash/config/pipelines/nginx_config.conf"], :thread=>"#<Thread:0x5bf4413d /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:134 run>"}
[2024-09-24T08:33:21,390][INFO ][logstash.javapipeline    ][service_stamped_json_logs] Pipeline Java execution initialization time {"seconds"=>1.48}
[2024-09-24T08:33:21,450][INFO ][logstash.inputs.file     ][service_stamped_json_logs] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_d883144359d3b4f516b37dba51fab2a2", :path=>["/var/log/nginx/access.log"]}
[2024-09-24T08:33:21,457][INFO ][logstash.javapipeline    ][service_stamped_json_logs] Pipeline started {"pipeline.id"=>"service_stamped_json_logs"}
[2024-09-24T08:33:21,465][INFO ][filewatch.observingtail  ][service_stamped_json_logs][5c3dcf575b397bd3a978dec84ce3504ca94369fa33799bf34ff0e85f4bbd279d] START, creating Discoverer, Watch with file and sincedb collections
[2024-09-24T08:33:21,501][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:service_stamped_json_logs], :non_running_pipelines=>[]}
