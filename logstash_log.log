Using bundled JDK: /usr/share/logstash/jdk
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/concurrent-ruby-1.1.9/lib/concurrent-ruby/concurrent/executor/java_thread_pool_executor.rb:13: warning: method redefined; discarding old to_int
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/concurrent-ruby-1.1.9/lib/concurrent-ruby/concurrent/executor/java_thread_pool_executor.rb:13: warning: method redefined; discarding old to_f
Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties
[2024-09-24T11:25:37,277][INFO ][logstash.runner          ] Log4j configuration path used is: /usr/share/logstash/config/log4j2.properties
[2024-09-24T11:25:37,285][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"8.12.2", "jruby.version"=>"jruby 9.4.5.0 (3.1.4) 2023-11-02 1abae2700f OpenJDK 64-Bit Server VM 17.0.10+7 on 17.0.10+7 +indy +jit [x86_64-linux]"}
[2024-09-24T11:25:37,288][INFO ][logstash.runner          ] JVM bootstrap flags: [-XX:+HeapDumpOnOutOfMemoryError, -Dlogstash.jackson.stream-read-constraints.max-number-length=10000, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, -Djruby.regexp.interruptible=true, --add-opens=java.base/java.security=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, -Dio.netty.allocator.maxOrder=11, -Dlog4j2.isThreadContextMapInheritable=true, -Xms1g, -Dlogstash.jackson.stream-read-constraints.max-string-length=200000000, -Djdk.io.File.enableADS=true, -Dfile.encoding=UTF-8, --add-opens=java.base/java.io=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, -Djruby.compile.invokedynamic=true, -Xmx1g, -Djava.security.egd=file:/dev/urandom, -Djava.awt.headless=true, -Dls.cgroup.cpuacct.path.override=/, -Dls.cgroup.cpu.path.override=/, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED]
[2024-09-24T11:25:37,290][INFO ][logstash.runner          ] Jackson default value override `logstash.jackson.stream-read-constraints.max-string-length` configured to `200000000`
[2024-09-24T11:25:37,290][INFO ][logstash.runner          ] Jackson default value override `logstash.jackson.stream-read-constraints.max-number-length` configured to `10000`
[2024-09-24T11:25:37,316][INFO ][logstash.settings        ] Creating directory {:setting=>"path.queue", :path=>"/usr/share/logstash/data/queue"}
[2024-09-24T11:25:37,356][INFO ][logstash.settings        ] Creating directory {:setting=>"path.dead_letter_queue", :path=>"/usr/share/logstash/data/dead_letter_queue"}
[2024-09-24T11:25:37,981][INFO ][logstash.agent           ] No persistent UUID file found. Generating new UUID {:uuid=>"80b909af-a1ff-4b23-a7f2-74d513798d70", :path=>"/usr/share/logstash/data/uuid"}
[2024-09-24T11:25:39,759][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600, :ssl_enabled=>false}
[2024-09-24T11:25:41,058][INFO ][org.reflections.Reflections] Reflections took 156 ms to scan 1 urls, producing 132 keys and 468 values
[2024-09-24T11:25:41,987][INFO ][logstash.javapipeline    ] Pipeline `service_stamped_json_logs` is configured with `pipeline.ecs_compatibility: v8` setting. All plugins in this pipeline will default to `ecs_compatibility => v8` unless explicitly configured otherwise.
[2024-09-24T11:25:42,084][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//elasticsearch:9200"]}
[2024-09-24T11:25:42,717][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elasticsearch:9200/]}}
[2024-09-24T11:25:43,195][WARN ][logstash.outputs.elasticsearch][service_stamped_json_logs] Restored connection to ES instance {:url=>"http://elasticsearch:9200/"}
[2024-09-24T11:25:43,272][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Elasticsearch version determined (8.12.2) {:es_version=>8}
[2024-09-24T11:25:43,272][WARN ][logstash.outputs.elasticsearch][service_stamped_json_logs] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>8}
[2024-09-24T11:25:43,448][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Not eligible for data streams because config contains one or more settings that are not compatible with data streams: {"index"=>"nginx-logs-%{+YYYY.MM.dd}"}
[2024-09-24T11:25:43,450][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Data streams auto configuration (`data_stream => auto` or unset) resolved to `false`
[2024-09-24T11:25:43,510][WARN ][logstash.filters.grok    ][service_stamped_json_logs] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-09-24T11:25:43,567][INFO ][logstash.outputs.elasticsearch][service_stamped_json_logs] Using a default mapping template {:es_version=>8, :ecs_compatibility=>:v8}
[2024-09-24T11:25:43,750][WARN ][logstash.javapipeline    ][service_stamped_json_logs] 'pipeline.ordered' is enabled and is likely less efficient, consider disabling if preserving event order is not necessary
[2024-09-24T11:25:43,787][INFO ][logstash.javapipeline    ][service_stamped_json_logs] Starting pipeline {:pipeline_id=>"service_stamped_json_logs", "pipeline.workers"=>1, "pipeline.batch.size"=>1, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1, "pipeline.sources"=>["/usr/share/logstash/config/pipelines/nginx_config.conf"], :thread=>"#<Thread:0x2596e9bf /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:134 run>"}
[2024-09-24T11:25:44,813][INFO ][logstash.javapipeline    ][service_stamped_json_logs] Pipeline Java execution initialization time {"seconds"=>1.02}
[2024-09-24T11:25:44,843][INFO ][logstash.inputs.file     ][service_stamped_json_logs] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_7447517cb9c285e1378946e63e786a5e", :path=>["/var/log/nginx/access1.log"]}
[2024-09-24T11:25:44,860][INFO ][logstash.javapipeline    ][service_stamped_json_logs] Pipeline started {"pipeline.id"=>"service_stamped_json_logs"}
[2024-09-24T11:25:44,864][INFO ][filewatch.observingtail  ][service_stamped_json_logs][37352f94b37e5a82dadb90f3bf958515fe8121e16a5af5cb2560ea13b1260a80] START, creating Discoverer, Watch with file and sincedb collections
[2024-09-24T11:25:44,871][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:service_stamped_json_logs], :non_running_pipelines=>[]}
